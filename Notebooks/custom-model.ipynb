{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(output_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):\n",
    "        super().compile(*args, **kwargs)\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "\n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake_data = self.generator(noise, training=False)\n",
    "            # Get discriminator predictions for real and fake data\n",
    "            real_output = self.discriminator(real_data, training=True)\n",
    "            fake_output = self.discriminator(fake_data, training=True)\n",
    "            d_loss_real = self.d_loss(tf.ones_like(real_output), real_output)\n",
    "            d_loss_fake = self.d_loss(tf.zeros_like(fake_output), fake_output)\n",
    "            total_d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        d_grads = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "            fake_data = self.generator(noise, training=True)\n",
    "            fake_output = self.discriminator(fake_data, training=False)\n",
    "            total_g_loss = self.g_loss(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "        g_grads = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "data_dim = 10  # Adjust this based on your tabular data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the model\n",
    "generator = build_generator(latent_dim, data_dim)\n",
    "discriminator = build_discriminator(data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "d_opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabgan = TabularGAN(generator, discriminator, latent_dim)\n",
    "tabgan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
